import os

from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider

HTTP = HTTPRemoteProvider()

localrules: download_podar, download_podar_reads

rule all:
    input:
        'outputs/gather.reads.x.podar.csv',
        'data/kraken_db/database.idx',
        'data/kraken_db/database.jdb',
        'data/kraken_db/database.kdb',
        'outputs/sbt/kraken-k21.dist',
        'data/podar-reference-genomes-updated-2017.05.20.tar.gz',
#        'data/CAMI_medium.tar',
#        'data/CAMI_high.tar'


rule download_podar:
    output: 'data/podar-reference-genomes-updated-2017.05.20.tar.gz'
    input: HTTP.remote('files.osf.io/v1/resources/vk4fa/providers/osfstorage/59203210b83f69024d8eb9aa')
    shell: '''
        mkdir -p data/podar
        cp '{input}' {output}
        tar xf {output} -C data/podar
    '''

rule download_cami:
    output: 'data/CAMI_{complexity}.tar'
    input: HTTP.remote('s3-eu-west-1.amazonaws.com/cami-data-eu/CAMI_{complexity}.tar')
    shell: '''
        mkdir -p data
        mv '{input}' {output}
    '''

rule build_index:
    output: 'outputs/podar-genomes-k{ksize}.sbt.json'
    input: expand('outputs/sigs/podar/{contig}.fa.sig', contig=range(0, 64))
    params:
        ksize='{ksize}',
        sbt_name='outputs/podar-genomes-k{ksize}'
    conda: 'envs/sourmash.yaml'
    shell: """
        sourmash index -k {params.ksize} {params.sbt_name} {input}
    """

rule compute_sig:
    output: 'outputs/sigs/{db}/{filename}.fa.sig'
    input: 'data/{db}/{filename}.fa'
    conda: 'envs/sourmash.yaml'
    shell: '''
        sourmash compute --scaled 10000 \
                         -k 21,31,41,51 \
                         --name-from-first \
                         -o {output} \
                         {input}
    '''

rule compute_reads_sig:
    output: 'outputs/sigs/podar-reads.10k.sig'
    input: 'data/podar-reads.fastq.gz'
    conda: 'envs/sourmash.yaml'
    shell: '''
        sourmash compute --scaled 10000 \
                         -k 21,31,41,51 \
                         --name-from-first \
                         -o {output} \
                         {input}
    '''

rule run_gather:
    output: 'outputs/gather.reads.x.podar.csv'
    input:
        sig='outputs/sigs/podar-reads.10k.sig',
        sbt='outputs/podar-genomes-k31.sbt.json'
    conda: 'envs/sourmash.yaml'
    shell: """
        mkdir -p `dirname {output}`
        sourmash gather {input.sig} \
                        {input.sbt} \
                        -o {output}
    """

rule download_podar_reads:
    output: 'data/podar-reads.fastq.gz'
    conda: 'envs/sourmash.yaml'
    shell: """
        fastq-dump -Z --gzip --skip-technical \
                   --readids \
                   --read-filter pass \
                   --dumpbase \
                   --split-files \
                   --clip \
                   -A SRR606249 > {output}
    """

####
#
####

rule download_sourmash_dbs:
    output: 'data/sourmash/{database}-k{ksize}.sbt.json'
    input: HTTP.remote('s3-us-west-1.amazonaws.com/spacegraphcats.ucdavis.edu/microbe-{database}-sbt-k{ksize}-2017.05.09.tar.gz')
    shell: '''
        tar xf {input} -C data/sourmash
    '''

####
# Rules for downloading i100
#  http://www.bork.embl.de/~mende/simulated_data/
#
####

rule download_i100:
    output: 'data/i100/{}.fq.gz'
    input: HTTP.remote('www.bork.embl.de/~mende/simulated_data/{}.fq.gz')
    shell: '''
        mkdir -p data/podar
        cp '{input}' {output}
        tar xf {output} -C data/podar
    '''


####
# These rules are for working with kraken.
#  - We download the data they use in their DB (seqs and taxonomy)
#  - Build the DB
####

rule download_kraken_genomes:
    output: dynamic("data/kraken_db/library/{kingdom}/{species}/{genome}.fna")
    conda: 'envs/kaiju_kraken.yaml'
    shell: """
        kraken-build --download-library bacteria --db data/kraken_db
        kraken-build --download-library viruses --db data/kraken_db
    """

rule download_kraken_taxonomy:
    output: "data/kraken_db/taxonomy/nodes.dmp"
    conda: 'envs/kaiju_kraken.yaml'
    shell: """
        kraken-build --download-taxonomy --db data/kraken_db
    """

rule build_kraken_db:
    input:
        "data/kraken_db/taxonomy/nodes.dmp",
        dynamic("data/kraken_db/library/{kingdom}/{species}/{genome}.fna")
    output:
        'data/kraken_db/database.idx',
        'data/kraken_db/database.jdb',
        'data/kraken_db/database.kdb'
    conda: 'envs/kaiju_kraken.yaml'
    threads: 16
    shell: """
        kraken-build --build --threads {threads} --db data/kraken_db
    """

####
# Since we still have the data for the kraken DB, we use it to build an SBT too.
#  - Calculate signatures for each dataset
#  - Compare to build a distance matrix
#  - Use fastcluster to build a tree skeleton
#  - Use the skeleton to build the SBT
####

rule compute_kraken_sigs:
    output: 'outputs/sigs/kraken/{kingdom}/{species}/{genome}.fna.sig'
    input: "data/kraken_db/library/{kingdom}/{species}/{genome}.fna"
    conda: 'envs/sourmash.yaml'
    shell: '''
        sourmash compute --scaled 10000 \
                         -k 21,31,41,51 \
                         --name-from-first \
                         -o {output} \
                         {input}
    '''

rule build_distance_matrix:
    output: 'outputs/sbt/kraken-k{ksize}.dist'
    input: dynamic("outputs/sigs/kraken/{kingdom}/{species}/{genome}.fna.sig")
    params: ksize="{ksize}"
    conda: 'envs/sourmash.yaml'
    shell: '''
        sourmash compare -k {params.ksize} --dna --output {output} {input}
    '''

###
#
###

rule kraken_podar_query:
    input:
        'data/kraken_db/database.jdb',
        'data/kraken_db/database.kdb',
        kraken_idx='data/kraken_db/database.idx',
        podar_reads='data/podar-reads.fastq.gz'
    output: 'outputs/queries/kraken.out'
    conda: 'envs/kaiju_kraken.yaml'
    threads: 16
    shell: """
        kraken --db `dirname {input.kraken_idx}` --fastq-input --gzip-compressed --preload --threads {threads} {input.podar_reads} > {output}
    """

rule sourmash_gather_podar_query:
    input:
        refseq_db='data/sourmash/refseq-k31.sbt.json',
        genbank_db='data/sourmash/genbank-k31.sbt.json',
        podar_sigs='outputs/sigs/podar-reads.10k.sig'
    output: 'outputs/queries/gather.out'
    conda: 'envs/sourmash.yaml'
    threads: 1
    shell: """
        sourmash gather -k 31 \
                        --scaled 10000 \
                        {input.podar_sigs} \
                        {input.refseq_db} \
                        {input.genbank_db} \
                        -o {output}
    """
